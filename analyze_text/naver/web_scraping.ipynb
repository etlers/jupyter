{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://en.wikipedia.org/wiki/Kevin_Bacon\")\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "for link in bs.findAll(\"a\"):\n",
    "    if \"href\" in link.attrs:\n",
    "        print(link.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_headline = []\n",
    "\n",
    "def get_news_headline(page):\n",
    "    \n",
    "    def except_word(headline):\n",
    "        result_tf = False\n",
    "        \n",
    "        list_word = [\"이혼\", \"결혼\"]\n",
    "        for word in list_word:\n",
    "            if word in headline:\n",
    "                result_tf = True\n",
    "                break\n",
    "        \n",
    "        return result_tf\n",
    "        \n",
    "    base_url = f\"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101&page={page}\"\n",
    "    response = requests.get( base_url, headers={\"User-agent\": \"Mozilla/5.0\"} )\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    list_news = str(soup.findAll(\"div\", {\"id\": \"main_content\"})).split('\\n')\n",
    "    for row in list_news:\n",
    "        if '<a class=\"cluster_text_headline nclicks' in row:\n",
    "            filtered_row = row.replace('<a class=\"cluster_text_headline nclicks(cls_eco.clsart)\"',\"\").replace(\"</a>\",\"\")\n",
    "            str_href = filtered_row.split(\">\")[0].strip()\n",
    "            str_headline = filtered_row.split(\">\")[1].strip()\n",
    "            if except_word(str_headline): continue\n",
    "            list_headline.append(str_headline)\n",
    "            \n",
    "for idx in range(5):\n",
    "    get_news_headline(idx+1)\n",
    "    \n",
    "print(list_headline)\n",
    "\n",
    "df_headline = pd.DataFrame(list_headline, columns=[\"헤드라인\"])\n",
    "print(df_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = f\"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101\"\n",
    "response = requests.get( base_url, headers={\"User-agent\": \"Mozilla/5.0\"} )\n",
    "soup = bs(response.text, 'html.parser')\n",
    "list_news = str(soup.findAll(\"div\", {\"id\": \"main_content\"})).split('\\n')\n",
    "for row in list_news:\n",
    "    if ('<span class=\"cluster_head_sub_topic\">' in row or\n",
    "        '<a class=\"cluster_text_headline nclicks(cls_eco.clsart)\"' in row or\n",
    "        '<div class=\"cluster_text_lede\">' in row or\n",
    "        '<a class=\"cluster_head_more nclicks(cls_eco.clstitle)\"' in row or\n",
    "        '<span class=\"cluster_head_sub_topic\">' in row):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def send_message_to_slack(msg):\n",
    "    msg = msg.replace('\"', \"'\").replace(\"/\",\"\")\n",
    "    url = \"https://hooks.slack.com/services/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\" \n",
    "    pyload = { \"text\" : msg } \n",
    "    response = requests.post(url, json=pyload)\n",
    "    print(response)\n",
    "\n",
    "def except_word(headline):\n",
    "    result_tf = False\n",
    "\n",
    "    list_word = [\n",
    "        \"이혼\", \"결혼\" \"부동산\", \"LH\", \"한국주택공사\", \"분양\", \"전세\", \"월세\", \"전월세\", \"민간임대\", \"공공임대\" ,\"다주택\", \n",
    "        \"사망\", \"산재\", \"상품권\", \"쿠폰\", \"유니폼\", \"아파트\", \"할인\", \"국토부\"\n",
    "    ]\n",
    "    for word in list_word:\n",
    "        if word in headline:\n",
    "            result_tf = True\n",
    "            break\n",
    "\n",
    "    return result_tf\n",
    "\n",
    "def replace_word(word):\n",
    "    list_remove = [\n",
    "        '<span class=\"cluster_head_sub_topic', \"</span>\", \"</a>\", \"</div>\", \"…\", \"\\'\"\n",
    "    ]\n",
    "    for val in list_remove:\n",
    "        word = word.replace(val, \"\")\n",
    "    word = word.replace(\"&amp;\",\"&\").replace(\"銀\",\"은행\").replace(\"▲\",\"\").replace(\"&gt;\",\">\").replace(\"&lt;\",\"<\")\n",
    "    word = word.replace(\"&eq;\",\"=\")\n",
    "\n",
    "    return word\n",
    "\n",
    "list_filtered = []\n",
    "\n",
    "def get_news():\n",
    "    base_url = f\"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101\"\n",
    "    response = requests.get( base_url, headers={\"User-agent\": \"Mozilla/5.0\"} )\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    list_div_class = [\n",
    "        \"_persist\",\n",
    "        \"section_body\",\n",
    "    ]\n",
    "    list_check_tag = [\n",
    "        '<div class=\"cluster_text_lede\">',\n",
    "        '<span class=\"cluster_head_sub_topic\">',\n",
    "        '<a class=\"cluster_thumb_link nclicks(cls_eco.clsart)\"',\n",
    "        '<a class=\"cluster_text_headline nclicks(cls_eco.clsart)\"',\n",
    "    ]\n",
    "\n",
    "    for div_class in list_div_class:\n",
    "        list_news = str(soup.findAll(\"div\", {\"class\": div_class})).split('\\n')\n",
    "        for row in list_news:\n",
    "            if except_word(row): continue\n",
    "            for tag in list_check_tag:\n",
    "                if tag in row:\n",
    "                    filtered_row = replace_word(row.replace(tag,\"\"))\n",
    "                    try:\n",
    "                        filtered_row = filtered_row.split('\">')[1]\n",
    "                    except:\n",
    "                        pass\n",
    "                    filtered_row = filtered_row.strip()\n",
    "                    if len(filtered_row) == 0: continue\n",
    "                    if (filtered_row[:1] == '\"' or filtered_row[:1] == '“'):\n",
    "                        filtered_row = filtered_row[1:]\n",
    "                    list_filtered.append(filtered_row)\n",
    "          \n",
    "for idx in range(5):\n",
    "    get_news()\n",
    "    time.sleep(10)\n",
    "    \n",
    "list_filtered = list(set(list_filtered))\n",
    "list_filtered = sorted(list_filtered)\n",
    "df_news = pd.DataFrame(list_filtered, columns=[\"헤드라인\"])\n",
    "\n",
    "msg = \"\"\n",
    "for row in list_filtered:\n",
    "    msg += row + \"\\n\"\n",
    "print(msg)\n",
    "print(len(msg))\n",
    "send_message_to_slack(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# https://hooks.slack.com/services/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\n",
    "\n",
    "import requests\n",
    "\n",
    "def send_message_to_slack(text): \n",
    "    url = \"https://hooks.slack.com/services/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\" \n",
    "    pyload = { \"text\" : text } \n",
    "    response = requests.post(url, json=pyload)\n",
    "    print(response)\n",
    "    \n",
    "send_message_to_slack(\"Test WebHook Message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
